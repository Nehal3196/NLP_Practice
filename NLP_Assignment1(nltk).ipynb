{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Assignment 1 - NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outlook.com\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "**Question 1.** Write a python program to find out the words after '@' from the below sentences with the use of regex.\n",
    "\n",
    "\"xyz@gmail.com\",\n",
    "\"abc@yahoo.com\",\n",
    "\"xyz@hotmail.com\",\n",
    "\"abc@ineuron.ai\",\n",
    "\"xyz@outlook.com\" \"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "def find_domain(mail_id):\n",
    "    a = re.split('@',mail_id)\n",
    "    return a[1]\n",
    "\n",
    "b = find_domain(\"xyz@outlook.com\")\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Delhi is the capital of India\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "**Question 2.** Write a python program with the use of regex to take out the word \"New\" from the following sentence.\n",
    "\n",
    "[\"New Delhi is the capital of India\"]\"\"\"\n",
    "\n",
    "import re\n",
    "def remove_word(sent,wrd):\n",
    "    a = re.sub(wrd,'',sent)\n",
    "    return a\n",
    "\n",
    "b = remove_word(\"New Delhi is the capital of India\",\"New\")\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in india,  people got affected with corona virus and  are died.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "**Question 3.** Create one python program in which you have to lowercase the sentence first and \n",
    "than delete digits from the following sentence.\n",
    "\n",
    "\"In India, 184 people got affected with Corona virus and 4 are died.\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "data = \"In India, 184 people got affected with Corona virus and 4 are died.\"\n",
    "\n",
    "def remove_digits(data):\n",
    "    data = data.lower()\n",
    "    reg = r'\\d'\n",
    "    a = re.sub(reg,'',data)\n",
    "    return a\n",
    "b = remove_digits(data)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming\n",
      "****************************************************************************************************\n",
      " i hop that , when i hav built up my sav , i wil be abl to travel to hawa .\n",
      " i hope that , when i have built up my save , i will be abl to travel to hawai .\n",
      " i hope that , when i have built up my save , i will be abl to travel to hawai .\n",
      "****************************************************************************************************\n",
      "lemmatization\n",
      "****************************************************************************************************\n",
      " i hope that , when i have built up my saving , i will be able to travel to hawai .\n",
      "****************************************************************************************************\n",
      "Tokenization\n",
      "****************************************************************************************************\n",
      "['I hope that, when I have built up my savings, I will be able to travel to Hawai.']\n",
      "['I', 'hope', 'that', ',', 'when', 'I', 'have', 'built', 'up', 'my', 'savings', ',', 'I', 'will', 'be', 'able', 'to', 'travel', 'to', 'Hawai', '.']\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "**Question 4.** Do stemming, lemmatization and tokenization from the following sentence.\n",
    "\n",
    "\"I hope that, when I have built up my savings, I will be able to travel to Hawai.\"\n",
    "\"\"\"\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "def stemming_lancaster(data):\n",
    "    lancaster = LancasterStemmer()\n",
    "    data = nltk.word_tokenize(data.lower())\n",
    "    stemmed = ''\n",
    "    for i in data:\n",
    "        stem_out = lancaster.stem(i)\n",
    "        stemmed = stemmed + ' '+ stem_out\n",
    "    return stemmed\n",
    "\n",
    "def stemming_porter(data):\n",
    "    porter = PorterStemmer()\n",
    "    data = nltk.word_tokenize(data.lower())\n",
    "    stemmed = ''\n",
    "    for i in data:\n",
    "        stem_out = porter.stem(i)\n",
    "        stemmed = stemmed + ' ' + stem_out\n",
    "    return stemmed\n",
    "\n",
    "def stemming_snowball(data):\n",
    "    snowball = SnowballStemmer('english')\n",
    "    data = nltk.word_tokenize(data.lower())\n",
    "    stemmed = ''\n",
    "    for i in data:\n",
    "        stem_out = snowball.stem(i)\n",
    "        stemmed = stemmed + ' ' + stem_out\n",
    "    return stemmed\n",
    "\n",
    "\n",
    "def stemming_by_type(stem_type,data):\n",
    "    if stem_type.lower() == 'lancaster':\n",
    "        return stemming_lancaster(data)\n",
    "    \n",
    "    if stem_type.lower() == 'porter':\n",
    "        return stemming_porter(data)\n",
    "    \n",
    "    if stem_type.lower() == 'snowball':\n",
    "        return stemming_snowball(data) \n",
    "    \n",
    "    return 'Invalid stemming type ' + stem_type\n",
    "    \n",
    "print('Stemming')\n",
    " \n",
    "print('*'*100)\n",
    "\n",
    "x = stemming_by_type('lancaster',\"I hope that, when I have built up my savings, I will be able to travel to Hawai.\")\n",
    "print(x) \n",
    "\n",
    "y = stemming_by_type('porter',\"I hope that, when I have built up my savings, I will be able to travel to Hawai.\")\n",
    "print(y) \n",
    "\n",
    "z = stemming_by_type('snowball',\"I hope that, when I have built up my savings, I will be able to travel to Hawai.\")\n",
    "print(z) \n",
    "\n",
    "print('*'*100)\n",
    "\n",
    "print('lemmatization')\n",
    "\n",
    "print('*'*100)\n",
    "\n",
    "def lemmatize_data(data):\n",
    "    data = nltk.word_tokenize(data.lower())\n",
    "    lemma = WordNetLemmatizer()\n",
    "    data_out = ''\n",
    "    for i in data:\n",
    "        lemma_out = lemma.lemmatize(i)\n",
    "        data_out = data_out + ' ' + lemma_out\n",
    "    return data_out\n",
    "\n",
    "l = lemmatize_data(\"I hope that, when I have built up my savings, I will be able to travel to Hawai.\")\n",
    "print(l)\n",
    "print('*'*100)\n",
    "\n",
    "print('Tokenization')\n",
    "\n",
    "print('*'*100)\n",
    "\n",
    "def tokenize_data(token_type,data):\n",
    "    if token_type.lower() == 'sentence':\n",
    "        return nltk.sent_tokenize(data)\n",
    "    if token_type.lower() == 'word':\n",
    "        return nltk.word_tokenize(data)\n",
    "    return 'Invalid tokenize type ' + token_type\n",
    "t1 = tokenize_data('sentence',\"I hope that, when I have built up my savings, I will be able to travel to Hawai.\")\n",
    "t2 = tokenize_data('word',\"I hope that, when I have built up my savings, I will be able to travel to Hawai.\")\n",
    "print(t1)\n",
    "print(t2)\n",
    "print('*'*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'l', 'N', 'n', 'y']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "**Question 5.** Create one python program from the following sentence.\n",
    "\n",
    "\"I love NLP, not you\"\n",
    "\n",
    "output : ['I', 'l', 'N', 'n', 'y']\n",
    "\"\"\"\n",
    "import string\n",
    "\n",
    "def first_letter(data):\n",
    "    punct = string.punctuation\n",
    "    data = nltk.word_tokenize(data)\n",
    "    out = []\n",
    "    for i in data:\n",
    "        if i not in punct:\n",
    "            out.append(i[0])\n",
    "    return out\n",
    "\n",
    "x = first_letter(\"I love NLP, not you\")\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
